# -*- coding: utf-8 -*-
"""Sentiment Analysis - CNN_LSTM(Using Arsas Dataset) -.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ok__CFkKGD5TwVblFiINI5wVTesrxRXB

**Importing the libraries needed**
"""

import numpy as np
import pandas as pd
import time


import matplotlib.pyplot as plt
import seaborn as sns

import re
import string

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

import gensim
from gensim.models import KeyedVectors

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import tensorflow as tf
from keras.models import Sequential
from tensorflow.keras.layers import SpatialDropout1D, Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam

import itertools
from numpy import loadtxt
from keras.models import load_model

import warnings
warnings.filterwarnings("ignore")

"""**Connecting to google drive**"""

from google.colab import drive
drive.mount("/content/gdrive")

""" 
**Uploading the dataset**
"""

path_data = "/content/gdrive/MyDrive/thesis/modified.csv"

Arsas = pd.read_csv(path_data ,sep='\t')

data = Arsas

"""**printing the first 3 rows of the data**"""

data.head(3)

"""**printing the shape of the dataset nbr of row and columns**"""

print("Data contient {} lignes et {} colonnes.".format(data.shape[0], data.shape[1]))

""" **printing the fiels with missed values**


"""

data.isnull().sum()

"""**printing the number of the duplicated rows**"""

print("On a  {} doublons dans Data.".format(data.duplicated().sum()))

data.drop_duplicates(inplace = True)

print("On a  {} doublons dans Data.".format(data.duplicated().sum()))

"""**checking the types of the fiels in the data**"""

data.dtypes

"""**function for printing the pie**"""

def pie(data,col):
    labels = data[col].value_counts().keys().tolist()
    n = len(labels)
    if n==2:
        colors = ['#66b3ff', '#fb3999']
    elif n==3:
        colors = ['#66b3ff', '#fb3999', '#ffcc99']
    elif n==4:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff"]
    elif n==5:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff",'#adcc99']
    elif n==6:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff",'#adcc99',"#db7f23"]
    
    fig1, f1 = plt.subplots()
    f1.pie(data[col].value_counts(), labels=labels, colors = colors, autopct='%1.1f%%',shadow=False, startangle=60) 
    f1.axis('equal')
    plt.tight_layout()
    plt.show()
    
def histo(data,col):
    plt.figure(figsize = (10, 8))
    sns.histplot(data=data, x=col, hue = data[col], fill=True)

"""**Counting the % of each classe**"""

data.Sentiment_label.value_counts(normalize = True)

"""**Printing the distribution of the classes**"""

pie(data, "Sentiment_label")

positive = data[data["Sentiment_label"] == "Positive"]
positive["sentiment"] = 1

mixed = data[data["Sentiment_label"] == "Mixed"]
mixed["sentiment"] = 2

neutral = data[data["Sentiment_label"] == "Neutral"]
neutral["sentiment"] = 3

negative = data[data["Sentiment_label"] == "Negative"]
negative["sentiment"] = 0

data = pd.concat([positive, mixed, neutral, negative], ignore_index = True)

print("data contient {} lignes.".format(data.shape[0]))

print("Positive contient {} lignes.".format(positive.shape[0]))

print("Negative contient {} lignes.".format(negative.shape[0]))

print("Mixed contient {} lignes.".format(mixed.shape[0]))

print("Neutral contient {} lignes.".format(neutral.shape[0]))

pie(data,"sentiment")

histo(data,"Sentiment_label")

"""**function to count the length of reviews**


"""

def compte_mots(phrase):
    return len(phrase.split())

data["len_review"] = data["Tweet_text"].apply(compte_mots)

"""**printing the max  length of the positive and negative reviews**"""

print("Le maximum de mots utilisé dans les reviews  est :", max(data['len_review']))
print("Le moyen de mots utilisé dans les reviews est :", np.mean(data['len_review']))

plt.figure(figsize=(10,9))

p1=sns.kdeplot(data['len_review'], hue = data['Sentiment_label'],  shade=True, color="r")

plt.xlim(0, 100)

"""**Deleting unused fields**"""

data.drop(['#Tweet_ID'], axis = 1, inplace = True)
data.head(3)

df = data
df.dtypes

"""**the function of the preprocessing**"""

def preprocessing(text):
     
    # ref: https://github.com/bakrianoo/aravec
    search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى",
              "\\",'\n', '\t','&quot;','?','؟','!']
    replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا",
               "","","","ي","",' ', ' ',' ',' ? ',' ؟ ', ' ! ']
    
    tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
    text = re.sub(tashkeel,"", text)
    
    longation = re.compile(r'(.)\1+')
    subst = r"\1\1"
    text = re.sub(longation, subst, text)
    
    text = re.sub(r"[^\w\s]", '', text)
    text = re.sub(r"[a-zA-Z]", '', text)
    text = re.sub(r"\d+", ' ', text)
    text = re.sub(r"\n+", ' ', text)
    text = re.sub(r"\t+", ' ', text)
    text = re.sub(r"\r+", ' ', text)
    text = re.sub(r"\s+", ' ', text)
    text = text.replace('وو', 'و')
    text = text.replace('يي', 'ي')
    text = text.replace('اا', 'ا')
    
    for i in range(0, len(search)):
        text = text.replace(search[i], replace[i])
    
    text = text.strip()
    
    return text

"""**preprocessing the reviews and printing the time spent**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data["Clean_reviews"] = data.Tweet_text.apply(lambda x: preprocessing(x))

"""**printing a review before and after preprocessing**"""

print('- Avant le prétraitement \n\n',data["Tweet_text"][4])
print("\n-----------------------------------------------\n")
print('- Après le prétraitement \n\n',data["Clean_reviews"][4])

"""**Saving the cleaned data in a csv file**"""

data.to_csv("cleaned_Arsas.csv")

"""**asigning the reviews and classes to a new variables**



"""

X = data.Clean_reviews
y=pd.get_dummies(data.sentiment)
# y = data.sentiment

"""**spliting the data to train and test set**


"""

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = 0.20, 
                                                    random_state = 42)

"""**printing the number of the train set and the test set**"""

print('Train set', X_train.shape)
print('Test set', X_test.shape)

from google.colab import drive
drive.mount('/content/gdrive')

"""**Uploading the fsttext pretrained word embedding with 150 dimension**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# target_word_vec = KeyedVectors.load_word2vec_format("/content/gdrive/MyDrive/thesis/cc.ar.150.vec", binary = False)

"""**tokenization of the reviews**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(X_train)

word_index = tokenizer.word_index
vocab_size = len(tokenizer.word_index) + 1

"""**making all reviews of the same length 70**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# MAX_SEQUENCE_LENGTH = 70
# 
# X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),
#                         maxlen = MAX_SEQUENCE_LENGTH)
# X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),
#                        maxlen = MAX_SEQUENCE_LENGTH)
# 
# print("Training X Shape:", X_train.shape)
# print("Testing X Shape:", X_test.shape)

"""**Construction of the embedding matrix**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding_matrix = np.zeros((vocab_size, 150))
# 
# for word, i in word_index.items():
#     if word in target_word_vec :
#         embedding_vector = target_word_vec[word]
#         if embedding_vector is not None:
#             embedding_matrix[i] = embedding_vector

embedding_matrix.shape[0] == vocab_size

"""**Creating the model**"""

model = Sequential()
embedding_layer = Embedding(vocab_size, 
                            150, 
                            weights = [embedding_matrix], 
                            input_length = MAX_SEQUENCE_LENGTH, 
                            trainable=False)
model.add(embedding_layer)
model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))
model.add(LSTM(64, dropout=0.2, return_sequences=True))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.2))
model.add(Dense(4, activation='softmax'))

model.compile(optimizer = Adam(learning_rate=0.001), 
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
print(model.summary())

"""**fitting the model to the dataset**"""

history = model.fit(X_train, y_train, validation_split=0.15, batch_size = 128, epochs=20, verbose=1, callbacks=[es])

"""**Evaluating the model**"""

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1,21)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(1,21)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

score = model.evaluate(X_test, y_test, verbose=1)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

y_pred = model.predict(X_test)

y_pred = (y_pred > 0.5)

print(classification_report(y_test, y_pred))

"""**function for creating confusion matrix**"""

def print_confusion_matrix(confusion_matrix, class_names, title='Confusion matrix', figsize = (6,6), fontsize=14):
    df_cm = pd.DataFrame(
        confusion_matrix, index=class_names, columns=class_names, 
    )
    fig = plt.figure(figsize=figsize)
    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt="d")
    except ValueError:
        raise ValueError("Confusion matrix values must be integers.")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.title(title, fontsize=20)
    
    return fig

"""**printing the confusion matrix**"""

from sklearn.metrics import multilabel_confusion_matrix

cnf_matrix = multilabel_confusion_matrix(y_test, y_pred).reshape(4*1, -1)
classes = [str(x) for x in list(y_test.columns.values.tolist())]

print_confusion_matrix(cnf_matrix, classes);