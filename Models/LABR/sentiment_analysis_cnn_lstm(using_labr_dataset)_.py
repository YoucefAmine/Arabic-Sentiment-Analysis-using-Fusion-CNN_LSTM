# -*- coding: utf-8 -*-
"""Sentiment Analysis - CNN_LSTM(Using LABR Dataset) -.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m_hRhOraUlvYKkJNUEy3eRZHkkqAB-7p

**Importing the libraries needed**
"""

import numpy as np
import pandas as pd
import time


import matplotlib.pyplot as plt
import seaborn as sns

import re
import string

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

import gensim
from gensim.models import KeyedVectors

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import tensorflow as tf
from keras.models import Sequential
from tensorflow.keras.layers import SpatialDropout1D, Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam

import itertools
from numpy import loadtxt
from keras.models import load_model

import warnings
warnings.filterwarnings("ignore")

"""**Connecting to google drive**"""

from google.colab import drive
drive.mount("/content/gdrive")

""" 
**Uploading the dataset**
"""

# path_data = "/content/gdrive/MyDrive/thesis/LABR.tsv"

# LABR = pd.read_csv(path_data, sep='\t')

path_data = "/content/gdrive/MyDrive/thesis/LABR.xlsx"

LABR = pd.read_excel(path_data)

data = LABR

"""**printing the first 3 rows of the data**"""

data.head(3)

"""**printing the shape of the dataset nbr of row and columns**"""

print("Data contient {} lignes et {} colonnes.".format(data.shape[0], data.shape[1]))

""" **printing the fiels with missed values**


"""

data.isnull().sum()

"""**printing the number of the duplicated rows**"""

print("On a  {} doublons dans Data.".format(data.duplicated().sum()))

data.drop_duplicates(inplace = True)

print("On a  {} doublons dans Data.".format(data.duplicated().sum()))

"""**checking the types of the fiels in the data**"""

data.dtypes

"""**function for printing the pie**"""

def pie(data,col):
    labels = data[col].value_counts().keys().tolist()
    n = len(labels)
    if n==2:
        colors = ['#66b3ff', '#fb3999']
    elif n==3:
        colors = ['#66b3ff', '#fb3999', '#ffcc99']
    elif n==4:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff"]
    elif n==5:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff",'#adcc99']
    elif n==6:
        colors = ['#66b3ff', '#fb3999', '#ffcc99',"#66f3ff",'#adcc99',"#db7f23"]
    
    fig1, f1 = plt.subplots()
    f1.pie(data[col].value_counts(), labels=labels, colors = colors, autopct='%1.1f%%',shadow=False, startangle=60) 
    f1.axis('equal')
    plt.tight_layout()
    plt.show()
    
def histo(data,col):
    plt.figure(figsize = (10, 8))
    sns.histplot(data=data, x=col, hue = data[col], fill=True)

"""**Counting the % of each classe**"""

data.rating.value_counts(normalize = True)

"""**Printing the distribution of the classes**"""

pie(data, "rating")

"""**Repartitionning the data to 2 classes**"""

positive_reviews = data[data["rating"] > 3]
positive_reviews["sentiment"] = 1

negative_reviews = data[data["rating"] < 3]
negative_reviews["sentiment"] = 0

data = pd.concat([positive_reviews, negative_reviews], ignore_index = True)

""" **printing the number of rows in both classes**"""

print("data contient {} lignes.".format(data.shape[0]))

print("Positive_reviews contient {} lignes.".format(positive_reviews.shape[0]))

print("Negative_reviews contient {} lignes.".format(negative_reviews.shape[0]))

"""**printing the new distribution of the data**"""

pie(data,"sentiment")

"""**printing the new distribution in histogramme**"""

histo(data,"sentiment")

"""**function to count the length of reviews**


"""

def compte_mots(phrase):
    return len(str(phrase).split())

data["len_review"] = data["review"].apply(compte_mots)
positive_reviews['len_review'] = positive_reviews["review"].apply(compte_mots)
negative_reviews['len_review'] = negative_reviews["review"].apply(compte_mots)

"""**printing the max  length of the positive and negative reviews**"""

print("Le maximum de mots utilisé dans les reviews positives est :", max(positive_reviews.len_review))
print("Le moyen de mots utilisé dans les reviews positives est :", np.mean(positive_reviews.len_review))
print("-------------------------------------------------------------------------------------------------------")
print("Le maximum de mots utilisé dans les reviews négatives est :", max(negative_reviews.len_review))
print("Le moyen de mots utilisé dans les reviews négatives est :", np.mean(negative_reviews.len_review))

plt.figure(figsize=(10,9))

p1=sns.kdeplot(positive_reviews['len_review'], hue = data['sentiment'],  shade=True, color="r")
p1=sns.kdeplot(negative_reviews['len_review'], shade=True, color="b")

plt.xlim(0, 400)

"""**Deleting unused fields**"""

data.drop(['rating', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3'], axis = 1, inplace = True)
data.head(3)

df = data

"""**the function of the preprocessing**"""

def preprocessing(x):
    x = re.sub('@[^\s]+', ' ', x)
    x = re.sub('((www\.[^\s]+)|(https?://[^\s]+))',' ',x)
    
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030""]+", flags=re.UNICODE)
    emoji_pattern.sub(r'', x)
    
    ar_punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ#'''
    en_punctuations = string.punctuation
    punctuations = ar_punctuations + en_punctuations
    x = x.translate(str.maketrans('', '', punctuations))
 
    arabic_diacritics = re.compile(""" ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida
                         """, re.VERBOSE)
    x = re.sub(arabic_diacritics, '', str(x)) 
    
#     x = re.sub("[إأآا]", "ا", x)
#     x = re.sub("ى", "ي", x)
#     x = re.sub("ة", "ه", x)
#     x = re.sub("گ", "ك", x)
#     x = re.sub(r'(.)\1+', r'\1', x)
    
    return x

"""**preprocessing the reviews and printing the time spent**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data["Clean_reviews"] = data.review.apply(lambda x: preprocessing(str(x)))

"""**printing a review before and after preprocessing**"""

print('- Avant le prétraitement \n\n',data["review"][4])
print("\n-----------------------------------------------\n")
print('- Après le prétraitement \n\n',data["Clean_reviews"][4])

"""**Saving the cleaned data in a csv file**"""

data.to_csv("cleaned_labr.csv")

"""**asigning the reviews and classes to a new variables**



"""

X = data.Clean_reviews
y = data.sentiment

"""**spliting the data to train and test set**


"""

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = 0.20, 
                                                    random_state = 42)

"""**printing the number of the train set and the test set**"""

print('Train set', X_train.shape)
print('Test set', X_test.shape)

from google.colab import drive
drive.mount('/content/gdrive')

"""**Uploading the fsttext pretrained word embedding with 150 dimension**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# target_word_vec = KeyedVectors.load_word2vec_format("/content/gdrive/MyDrive/thesis/cc.ar.150.vec", binary = False)

"""**tokenization of the reviews**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(X_train)

word_index = tokenizer.word_index
vocab_size = len(tokenizer.word_index) + 1

"""**making all reviews of the same length 3456**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# MAX_SEQUENCE_LENGTH = 3456
# 
# X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),
#                         maxlen = MAX_SEQUENCE_LENGTH)
# X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),
#                        maxlen = MAX_SEQUENCE_LENGTH)
# 
# print("Training X Shape:", X_train.shape)
# print("Testing X Shape:", X_test.shape)

"""**Construction of the embedding matrix**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding_matrix = np.zeros((vocab_size, 150))
# 
# for word, i in word_index.items():
#     if word in target_word_vec :
#         embedding_vector = target_word_vec[word]
#         if embedding_vector is not None:
#             embedding_matrix[i] = embedding_vector

embedding_matrix.shape[0] == vocab_size

"""**Creating the model**"""

model = Sequential()
embedding_layer = Embedding(vocab_size, 
                            150, 
                            weights = [embedding_matrix], 
                            input_length = MAX_SEQUENCE_LENGTH, 
                            trainable=False)
model.add(embedding_layer)
model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))
model.add(LSTM(64, dropout=0.2, return_sequences=True))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer = Adam(learning_rate=0.001), 
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
print(model.summary())

"""**fitting the model to the dataset**"""

history = model.fit(X_train, y_train, validation_split=0.15, batch_size = 128, epochs=20, verbose=1, callbacks=[es])

"""**Evaluating the model**"""

loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1,21)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(1,21)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

score = model.evaluate(X_test, y_test, verbose=1)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

def decode_sentiment(score):
    return 1 if score>0.5 else 0

scores = model.predict(X_test, verbose=1)

y_pred = [decode_sentiment(x) for x in scores]

print(classification_report(y_test, y_pred))

"""**function for creating confusion matrix**"""

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """

    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=20)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, fontsize=13)
    plt.yticks(tick_marks, classes, fontsize=13)

    fmt = '.2f'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label', fontsize=17)
    plt.xlabel('Predicted label', fontsize=17)

"""**printing the confusion matrix**"""

cnf_matrix = confusion_matrix(y_test.to_list(), y_pred)
plt.figure(figsize=(6,6))
plot_confusion_matrix(cnf_matrix, classes=y_test.unique(), title="Confusion matrix")
plt.show()